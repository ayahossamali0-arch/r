import os
import json
import re
import time
import threading
from dotenv import load_dotenv
from openai import OpenAI
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
from typing import List, Tuple

# ==============================
# 0ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
# ==============================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if OPENAI_API_KEY is None:
    raise ValueError("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ OPENAI_API_KEY ÙÙŠ Ù…Ù„Ù .env")

client = OpenAI(api_key=OPENAI_API_KEY)

TOP_K = 5
SIMILARITY_THRESHOLD = 0.55
EMBED_MODEL_NAME = "intfloat/multilingual-e5-large"
DATA_FILE = os.path.join(os.path.dirname(__file__), "kk.json")

# ==============================
# 1ï¸âƒ£ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¨Ù†Ø§Ø¡ FAISS
# ==============================
def load_data():
    if not os.path.exists(DATA_FILE):
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump([], f, ensure_ascii=False, indent=2)
    try:
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except json.JSONDecodeError:
        print("âš ï¸ kk.json ØªØ§Ù„Ù â€” Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ‡ÙŠØ¦Ø©...")
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump([], f, ensure_ascii=False, indent=2)
        return []

data = load_data()
texts = [item["content"] for item in data] if data else ["Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø¹Ø¯."]
model = SentenceTransformer(EMBED_MODEL_NAME)
text_embeddings = model.encode(texts, normalize_embeddings=True).astype("float32")

dimension = text_embeddings.shape[1]
index = faiss.IndexHNSWFlat(dimension, 32)
index.hnsw.efSearch = 64
index.add(text_embeddings)
last_modified = os.path.getmtime(DATA_FILE)

# ==============================
# 2ï¸âƒ£ ØªØ­Ø¯ÙŠØ« ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù„Ù€ FAISS
# ==============================
index_lock = threading.Lock()

def refresh_faiss_index_if_updated():
    global last_modified, index, text_embeddings, texts
    current_modified = os.path.getmtime(DATA_FILE)
    if current_modified != last_modified:
        print("ğŸ”„ Ø§ÙƒØªØ´Ø§Ù ØªØ¹Ø¯ÙŠÙ„ ÙÙŠ kk.json â€” ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙÙ‡Ø±Ø³...")
        new_data = load_data()
        if not new_data:
            print("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¹Ø¯.")
            return
        new_texts = [item["content"] for item in new_data]
        new_embeddings = model.encode(new_texts, normalize_embeddings=True).astype("float32")
        new_index = faiss.IndexHNSWFlat(new_embeddings.shape[1], 32)
        new_index.hnsw.efSearch = 64
        new_index.add(new_embeddings)
        with index_lock:
            index = new_index
            text_embeddings = new_embeddings
            texts = new_texts
            last_modified = current_modified
        print(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙÙ‡Ø±Ø³ ({len(texts)} Ø¹Ù†Ø§ØµØ±).")

# ==============================
# 3ï¸âƒ£ ØªØ¬Ù‡ÙŠØ² Ø§Ù„Ù†ØµÙˆØµ ÙˆØ§Ù„Ù†ÙˆØ§ÙŠØ§
# ==============================
ARABIC_DIACRITICS = re.compile(r"[Ù‹ÙŒÙÙÙÙÙ‘Ù’Ù€]")

def normalize_arabic(text: str) -> str:
    text = re.sub(ARABIC_DIACRITICS, "", text)
    text = text.replace("Ø¢", "Ø§").replace("Ø£", "Ø§").replace("Ø¥", "Ø§")
    text = text.replace("Ù‰", "ÙŠ").replace("Ø¤", "Ùˆ").replace("Ø¦", "ÙŠ")
    text = re.sub(r"[^\w\s\u0600-\u06FF]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# ==============================
# ğŸ”¥ Ø°ÙƒØ§Ø¡ ÙÙ‡Ù… Ø§Ù„Ù†ÙˆØ§ÙŠØ§
# ==============================
def detect_special_intent(text: str) -> Tuple[bool, str]:
    text_low = text.lower().strip()

    greeting_patterns = [
        r"Ù…Ø±Ø­Ø¨", r"Ø§Ù‡Ù„Ø§", r"Ø§Ù„Ø³Ù„Ø§Ù…", r"Ù‡Ù„Ø§", r"ØµØ¨Ø§Ø­ Ø§Ù„Ø®ÙŠØ±", r"Ù…Ø³Ø§Ø¡ Ø§Ù„Ø®ÙŠØ±",
        r"hi", r"hello", r"hey"
    ]
    for p in greeting_patterns:
        if re.search(p, text_low): return True, "greeting"

    farewell_patterns = [
        r"Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©", r"Ø³Ù„Ø§Ù…", r"Ø¨Ø§ÙŠ", r"ÙˆØ¯Ø§Ø¹", r"Ø§Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡", r"goodbye", r"bye"
    ]
    for p in farewell_patterns:
        if re.search(p, text_low): return True, "farewell"

    thanks_patterns = [
        r"Ø´ÙƒØ±", r"Ù…Ù…Ù†ÙˆÙ†", r"thx", r"thank", r"ÙŠØ³Ù„Ù…Ùˆ"
    ]
    for p in thanks_patterns:
        if re.search(p, text_low): return True, "thanks"

    praise_patterns = [
        r"Ø±Ø§Ø¦Ø¹", r"Ù…Ù…ØªØ§Ø²", r"Ø¬Ù…ÙŠÙ„", r"Ø°ÙƒÙŠ", r"Ø¹Ø¨Ù‚Ø±ÙŠ", r"Ø§Ø­Ø³Ù†Øª"
    ]
    for p in praise_patterns:
        if re.search(p, text_low): return True, "praise"

    hate_patterns = [
        r"Ø§ÙƒØ±Ù‡Ùƒ", r"Ø¨ÙƒØ±Ù‡Ùƒ", r"hate you", r"i hate you"
    ]
    for p in hate_patterns:
        if re.search(p, text_low): return True, "hate"

    return False, ""


# ==============================
# 4ï¸âƒ£ Ø³Ø¤Ø§Ù„ Ø§Ù„Ø·Ø§Ù„Ø¨ ÙˆØ§Ù„Ø±Ø¯
# ==============================
def rag_answer_final(user_question: str) -> str:
    threading.Thread(target=refresh_faiss_index_if_updated, daemon=True).start()

    # â— ÙƒØ´Ù Ø§Ù„Ù†ØµÙˆØµ ØºÙŠØ± Ø§Ù„Ù…ÙÙ‡ÙˆÙ…Ø© Ù‡Ù†Ø§
    n = user_question.strip()

    if len(n) < 3:
        return "â— Ø§Ù„Ù†Øµ ØºÙŠØ± ÙˆØ§Ø¶Ø­ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø³Ø¤Ø§Ù„Ùƒ."

    if re.fullmatch(r"[^A-Za-z0-9\u0600-\u06FF]+", n):
        return "â— Ø§Ù„Ù†Øµ ØºÙŠØ± ÙˆØ§Ø¶Ø­ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø³Ø¤Ø§Ù„Ùƒ."

    if re.fullmatch(r"[a-zA-Z]{3,}", n) and not re.search(r"(what|when|where|why|how)", n):
        return "â— Ø§Ù„Ù†Øµ ØºÙŠØ± ÙˆØ§Ø¶Ø­ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø³Ø¤Ø§Ù„Ùƒ."

    if re.fullmatch(r"[Ø£-ÙŠ]{3,}", n) and len(set(n)) <= 2:
        return "â— Ø§Ù„Ù†Øµ ØºÙŠØ± ÙˆØ§Ø¶Ø­ØŒ ÙŠØ±Ø¬Ù‰ ØªÙˆØ¶ÙŠØ­ Ø³Ø¤Ø§Ù„Ùƒ."

    # ğŸ‘‡ Ø±Ø¯ÙˆØ¯ Ø§Ù„Ù†ÙˆØ§ÙŠØ§
    intent_found, intent_type = detect_special_intent(user_question)
    if intent_found:
        responses = {
            "greeting": "ğŸ‘‹ Ø£Ù‡Ù„Ø§Ù‹ Ø¨Ùƒ! ÙƒÙŠÙ Ø£Ù‚Ø¯Ø± Ø£Ø³Ø§Ø¹Ø¯Ùƒ Ø§Ù„ÙŠÙˆÙ…ØŸ",
            "farewell": "ğŸ‘‹ Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©! Ø£ØªÙ…Ù†Ù‰ Ù„Ùƒ ÙŠÙˆÙ… Ø¬Ù…ÙŠÙ„.",
            "thanks": "ğŸ¤— Ù„Ø§ Ø´ÙƒØ± Ø¹Ù„Ù‰ ÙˆØ§Ø¬Ø¨! Ø¨Ø§Ù„ØªÙˆÙÙŠÙ‚ Ø¯Ø§Ø¦Ù…Ù‹Ø§.",
            "praise": "ğŸ™ Ø´ÙƒØ±Ø§Ù‹ Ø¹Ù„Ù‰ Ø°ÙˆÙ‚Ùƒ! Ø£Ù†Ø§ Ù‡Ù†Ø§ Ø¹Ù„Ø´Ø§Ù† Ø£Ø³Ø§Ø¹Ø¯Ùƒ.",
            "hate": "ğŸ™‚ Ø®Ù„ÙŠÙ†Ø§ Ù†Ø¨Ø¯Ø£ Ù…Ù† Ø¬Ø¯ÙŠØ¯â€¦ Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ ÙÙ‚Ø· â¤ï¸",
        }
        return responses.get(intent_type, "ğŸ™‚ Ø­Ø§Ø¶Ø±.")

    # ØªØ­Ø³ÙŠÙ† ÙÙ‡Ù… Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GPT
    try:
        ai_understanding = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Ø£Ø¹Ø¯ ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø´ÙƒÙ„ Ø£ÙˆØ¶Ø­ Ù„ØºØ±Ø¶ Ø§Ù„Ø¨Ø­Ø« Ø¯Ø§Ø®Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù†ØµÙŠØ©."},
                {"role": "user", "content": user_question},
            ]
        )
        refined_question = ai_understanding.choices[0].message.content.strip()
    except Exception:
        refined_question = user_question

    search_query = normalize_arabic(refined_question + " " + user_question)

    # Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    for item in load_data():
        content_norm = normalize_arabic(item.get("content", ""))
        file_url = item.get("file_url", "")
        if search_query in content_norm or (file_url and search_query in file_url.lower()):
            if file_url:
                return f"<img src='{file_url}' style='max-width:300px;'>"
            return item.get("content", "")

    # Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
    try:
        q_emb = model.encode([search_query], normalize_embeddings=True).astype("float32")
        with index_lock:
            distances, indices = index.search(q_emb, TOP_K)

        results = [(int(idx), float(1 - dist)) for idx, dist in zip(indices[0], distances[0])]
        results = [r for r in results if r[0] >= 0]
        results = sorted(results, key=lambda x: x[1], reverse=True)

        if not results or results[0][1] < SIMILARITY_THRESHOLD:
            return "â— Ù„Ø§ ØªÙˆØ¬Ø¯ Ø£ÙŠ Ù…Ø¹Ù„ÙˆÙ…Ø© Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©."

        best_idx = results[0][0]
        data_list = load_data()
        best_item = data_list[best_idx]

        if best_item.get("file_url"):
            return f"<img src='{best_item['file_url']}' style='max-width:300px;'>"

        try:
            optimized_answer = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Ù‚Ø¯Ù‘Ù… Ø¬ÙˆØ§Ø¨Ø§Ù‹ ÙˆØ§Ø¶Ø­Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ø·Ù‰."},
                    {"role": "user", "content": f"Ø§Ù„Ø³Ø¤Ø§Ù„: {user_question}\n\nØ§Ù„Ù†Øµ:\n{best_item.get('content', '')}"},
                ]
            )
            return optimized_answer.choices[0].message.content.strip()
        except Exception:
            return best_item.get("content", "")

    except Exception as e:
        return f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {e}"


# ==============================
# 5ï¸âƒ£ ØªØ´ØºÙŠÙ„ ØªÙØ§Ø¹Ù„ÙŠ
# ==============================
if __name__ == "__main__":
    print("ğŸ¤– Ù†Ø¸Ø§Ù… RAG Ø§Ù„Ø°ÙƒÙŠ Ø¬Ø§Ù‡Ø²! Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.")
    print("ğŸŸ¢ Ø§ÙƒØªØ¨ 'Ø®Ø±ÙˆØ¬' Ù„Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø©.")
    while True:
        user_q = input("ğŸ§‘â€ğŸ“: ").strip()
        if user_q.lower() in ["Ø®Ø±ÙˆØ¬", "exit", "quit"]:
            print("ğŸ¤–: Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø© ğŸ‘‹")
            break
        print("ğŸ¤–:", rag_answer_final(user_q))
